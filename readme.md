# ğŸ“¦ Multi Source ETL Pipeline
Data Engineer â€“ Take Home Test

## ğŸ“Œ Project Overview

Project ini merupakan implementasi pipeline ETL sederhana yang:

Mengambil data dari:

- Public Open API
- Dummy dataset (Netflix dataset)
- Mengirimkan data ke Cloud Storage gratis (Supabase Free Tier)
- Menggunakan Apache Airflow sebagai orchestrator

Menerapkan arsitektur data berlapis:
 - Raw
 - Staging
 - Mart

Pipeline ini dapat dijalankan ulang (reproducible) menggunakan Docker.

# ğŸ—ï¸ Architecture

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Public API  â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Extract     â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     RAW       â”‚  (JSON)
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   STAGING     â”‚  (Parquet - Cleaned)
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     MART      â”‚  (Aggregated Summary)
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Supabase    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# ğŸ§± Tech Stack

| Component  | Technology |
| ------------- | ------------- |
| Orchestration  | Apache Airflow (Dockerized)  |
| Storage  | Supabase Storage (Free Tier)  |
| Language | Python 3.12  |
| Format   | JSON & Parquet  |
| Containerization | Docker & Docker Compose  |

# ğŸ“‚ Project Structure

```
airflow-supabase-etl/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ multi_source_etl.py
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ etl_api.py
â”‚       â”œâ”€â”€ etl_netflix.py
â”‚       â”œâ”€â”€ transform_staging.py
â”‚       â”œâ”€â”€ transform_mart.py
â”‚       â””â”€â”€ load_to_supabase.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ mart/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ credentials.json
â””â”€â”€ README.md
```

# ğŸ“Š Data Layer Explanation

1ï¸âƒ£ RAW Layer

- Format: JSON
- Berisi data mentah hasil scraping
- Tidak ada transformasi
Contoh file:
- api_raw.json
- netflix_raw.json

Tujuan:
Menjaga reproducibility dan auditability data.

2ï¸âƒ£ STAGING Layer

- Format: Parquet
Transformasi sederhana dilakukan:

- Cleaning null values
- Normalisasi nama kolom
- Standarisasi format tanggal

Tujuan:
Menyiapkan data untuk analytics.

3ï¸âƒ£ MART Layer

- Format: Parquet
- Berisi data agregasi
Contoh output:
```
{"source":"api","total_records":100,"generated_at":"2026-02-24T06:30:00Z"}
{"source":"netflix","total_records":8807,"generated_at":"2026-02-24T06:30:00Z"}
```

Tujuan:
Menyediakan dataset siap konsumsi untuk reporting atau BI tools.

# ğŸš€ How To Run
```
1ï¸âƒ£ Clone Repository
git clone <your-repository-url>
cd airflow-supabase-etl
```

2ï¸âƒ£ Setup Environment Variables

Buat file .env:
```
SUPABASE_URL=https://xxxx.supabase.co
SUPABASE_KEY=your_supabase_key 


# Masukkan Fernet Key hasil generate di sini
# docker run --rm apache/airflow:2.9.0 python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=


# Masukkan Secret Key hasil generate di sini
# docker run --rm apache/airflow:2.9.0 python -c "import secrets; print(secrets.token_hex(32))"
AIRFLOW__WEBSERVER__SECRET_KEY=

```
3ï¸âƒ£ Build & Run Docker
```
docker compose up -d
```
4ï¸âƒ£ Initialize Airflow Database (First Time Only)
```
docker exec -it airflow-webserver airflow db init
```
5ï¸âƒ£ Access Airflow UI

Buka browser:
```
http://localhost:8080
```
Default login:
```
docker exec -it airflow-webserver bash

airflow users create \
  --username admin \
  --firstname admin \
  --lastname admin \
  --role Admin \
  --email admin@email.com \
  --password admin
```

6ï¸âƒ£ Trigger DAG

1. Aktifkan DAG multi_source_etl
2. Klik Trigger DAG
 
Pipeline akan berjalan:
```
extract_api
extract_netflix
        â†“
upload_raw
        â†“
transform_staging
        â†“
transform_mart
        â†“
upload_to_supabase
```

# â˜ï¸ Supabase Storage

Bucket: 
``` 
ListData 
```

Files generated:

- api_raw.json
- netflix_raw.json
- api_staging.parquet
- netflix_staging.parquet
- mart_summary.parquet

# ğŸ§  Design Decisions
Kenapa Supabase?

- Free tier
- Mudah diintegrasikan
- Tidak perlu setup cloud kompleks
- REST-based API sederhana

Kenapa Parquet?

- Columnar format
- Lebih efisien untuk analytics
- Compressed & performant
- Industry standard untuk data engineering

Kenapa Airflow?

- Industry standard orchestrator
- Mendukung scheduling
- Modular & scalable
- Mudah di-scale ke production

â­ Bonus Implemented

âœ” Transformasi data sederhana <br/>
âœ” Arsitektur raw â†’ staging â†’ mart <br/>
âœ” Orkestrasi dengan Airflow <br/>
âœ” Cloud storage integration <br/> 
âœ” Dockerized environment <br/>